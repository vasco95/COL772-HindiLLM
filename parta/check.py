"""
THIS FILE IS FOR EVALUATION PURPOSES ONLY. DO NOT EDIT THIS FILE.
Make sure your model.py code adheres to the specifications in the assignment writeup, and works with this file.
Check out run_model function for more details.
"""

import argparse
import json
from pathlib import Path
from typing import Any, Dict, List

import torch
import torch.nn as nn

import time

# Your code gets imported here.
from .model import load_model, collate_fn


def run_model(model: nn.Module, input_ids: List[torch.Tensor], vocab_size: int) -> List[Dict[str, torch.Tensor]]:
    """Run inference with the loaded model.

    Args:
        model: The neural network returned by `load_model`.
        input_ids: List of token tensors already on the correct device/order.

    Returns:
        A list of dictionaries containing model outputs. Each dictionary must contain logits and X_Final.

    Sample output:
    [
        {
            "logits": <A torch.Tensor of shape (T, |Vocab|)>,
        },.....
    ]
    """
    if torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    model.eval()
    model.to(device)

    outputs = []
    bsz = 16
    for st in range(0, len(input_ids), bsz):
        en = min(st + bsz, len(input_ids))
        batch = {
            "input_ids": [input_ids[i] for i in range(st, en)],
            "attention_mask": [torch.ones_like(input_ids[i]) for i in range(st, en)]
        }
        padded_batch = collate_fn(batch)
        padded_batch = {k: v.to(device) for k, v in padded_batch.items()}
        with torch.no_grad():
            logits = model(input_ids=padded_batch["input_ids"], attention_mask=padded_batch["attention_mask"])
        logits = logits.cpu()
        for i in range(en - st):
            outputs.append({
                "logits": logits[i][:len(batch["input_ids"][i])]
            })

    return outputs


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Part B Transformer runner")
    parser.add_argument("--config", type=Path, required=True, help="JSON config file")
    parser.add_argument(
        "--weights",
        type=Path,
        required=True,
        help="Checkpoint file to be passed to load_model",
    )
    parser.add_argument(
        "--input-ids",
        type=Path,
        required=True,
        help="Path to a JSON file containing a nested list of token IDs",
    )

    return parser.parse_args()


def read_config(path: Path) -> Dict[str, Any]:
    """Load experiment configuration from disk."""

    with path.open("r", encoding="utf-8") as handle:
        return json.load(handle)


def read_data(path: Path) -> Dict[str, Any]:
    """Convert a JSON list of sequences into individual LongTensors."""

    files = []
    for ff in path.iterdir():
        if ff.suffix == '.pth':
            files.append(ff)
    files = sorted(files, key=lambda x: int(x.name.split('.', 1)[0].strip()))

    input_ids = []
    outputs = []
    for dpath in files:
        # dpath = path / ff
        data = torch.load(dpath)
        input_ids.append(data['input_ids'])
        outputs.append(data['outputs'])

    return input_ids, outputs


def read_weights(path: Path) -> Dict[str, torch.Tensor]:
    """Load a serialized state dict (string -> tensor) from disk."""

    state_dict = torch.load(path, map_location="cpu")
    if not isinstance(state_dict, dict):
        raise TypeError("weights file must contain a state dict")
    return state_dict


def match(gold_outputs, model_outputs):
    assert len(gold_outputs) == len(model_outputs), f"Length mismatch: gold has {len(gold_outputs)}, model has {len(model_outputs)}"

    keys_to_match = list(gold_outputs[0])
    print(f"Matching {keys_to_match}")
    matches = {k: [] for k in keys_to_match}
    for i in range(len(gold_outputs)):
        for key in keys_to_match:
            gold = gold_outputs[i][key]
            model = model_outputs[i][key]
            matches[key].append(int(torch.allclose(gold, model, rtol=0.0, atol=1e-4)))

    print("================ Results ================")
    for key in matches:
        denom = len(matches[key])
        numer = sum(matches[key])
        print(f"{key}: {numer}/{denom} matches")


def main() -> None:
    args = parse_args()
    config = read_config(args.config)
    input_ids, gold_outputs = read_data(args.input_ids)
    state_dict = read_weights(args.weights)

    model = load_model(config=config, weights=state_dict)
    st = time.time()
    outputs = run_model(model=model, input_ids=input_ids, vocab_size=config["vocab_size"])
    en = time.time()
    match(gold_outputs=gold_outputs, model_outputs=outputs)
    print(f"Inference time: {en - st} seconds")


if __name__ == "__main__":
    main()
